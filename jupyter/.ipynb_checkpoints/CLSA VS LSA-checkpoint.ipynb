{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory \n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer # tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer # tf-idf\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "from scipy.linalg import svd \n",
    "from numpy import dot\n",
    "from nltk.corpus import stopwords # preprocessing\n",
    "from nltk.stem import PorterStemmer # preprocessing bahasa inggris\n",
    "from scipy.linalg import svd\n",
    "import numpy\n",
    "\n",
    "stemmer = StemmerFactory().create_stemmer()  # Object stemmer\n",
    "remover = StopWordRemoverFactory().create_stop_word_remover()  # objek stopword\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "def stemmerEN(text):\n",
    "    porter = PorterStemmer()\n",
    "    ## stop = set(stopwords.words('english')) #stopwods berguna untuk\n",
    "    factory = StopWordRemoverFactory()\n",
    "    stopwords = factory.get_stop_words()\n",
    "    text = text.lower()\n",
    "    text = [i for i in text.lower().split() if i not in stopwords]\n",
    "    text = ' '.join(text)\n",
    "    preprocessed_text = text.translate(translator)\n",
    "    text_stem = porter.stem(preprocessed_text)\n",
    "    return text_stem\n",
    "\n",
    "def preprocessing(text):\n",
    "    text = text.lower()\n",
    "    text_clean = remover.remove(text)\n",
    "    text_stem = stemmer.stem(text_clean)\n",
    "    text_stem = stemmerEN(text_stem)\n",
    "    return text_stem\n",
    "\n",
    "def splitParagraphIntoSentences(paragraph):\n",
    "    sentenceEnders = re.compile('[.!?]')\n",
    "    sentenceList = sentenceEnders.split(paragraph)\n",
    "    return sentenceList\n",
    "\n",
    "def TFIDF(doc) :  \n",
    "    factory = StopWordRemoverFactory()\n",
    "    stopwords = factory.get_stop_words()\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords,smooth_idf=False, norm=None)\n",
    "    X = tfidf_vectorizer.fit_transform(doc)\n",
    "    pd.DataFrame(X.toarray(), columns=tfidf_vectorizer.get_feature_names())\n",
    "    return X\n",
    "    #return pd.DataFrame(X.toarray(), columns=tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "def LSA(tfidf): \n",
    "    matrixAT=tfidf.toarray()\n",
    "    matrixA=np.transpose(matrixAT)\n",
    "    matrixAT=np.transpose(matrixA)\n",
    "    a = np.array(matrixAT)\n",
    "    b = np.array(matrixA)\n",
    "    aTa = dot(a,b)\n",
    "    tempSVD = np.array(aTa)\n",
    "    u, s, v = np.linalg.svd(tempSVD, full_matrices = False)\n",
    "    s = sp.diag(s)\n",
    "    loop = s[0]\n",
    "    for i in np.arange(np.size(loop)):\n",
    "        for j in np.arange(np.size(loop)):\n",
    "            s[i][j] = np.sqrt(dot(np.square(s[i][j]),np.square(v[i][j])))\n",
    "    return s\n",
    "\n",
    "def CLSA(tfidf):\n",
    "    matrixAT=tfidf.toarray()\n",
    "    matrixA=numpy.transpose(matrixAT)\n",
    "    matrixAT=numpy.transpose(matrixA)\n",
    "    a = np.array(matrixAT)\n",
    "    b = np.array(matrixA)\n",
    "    aTa = dot(a,b)\n",
    "    A = np.array(aTa)\n",
    "    u, s, v = np.linalg.svd(A, full_matrices = False)\n",
    "    s = sp.diag(s)\n",
    "    aa =v[0]\n",
    "    for i in np.arange(np.size(aa)):\n",
    "        av = np.average(v[i])\n",
    "        for j in np.arange(np.size(aa)):\n",
    "            if v[i][j] < av :\n",
    "                v[i][j] = 0\n",
    "    loop = s[0]\n",
    "    for i in np.arange(np.size(loop)):\n",
    "        for j in np.arange(np.size(loop)):\n",
    "            s[i][j] = np.sqrt(dot(np.square(s[i][j]),np.square(v[i][j])))\n",
    "    return s\n",
    "\n",
    "def summary_sentence(doc, result_TFIDF, types=''):\n",
    "    types = types.lower()\n",
    "    # try :\n",
    "    if (types == 'lsa') :\n",
    "        matrixAT=result_TFIDF.toarray()\n",
    "        matrixA=np.transpose(matrixAT)\n",
    "        matrixAT=np.transpose(matrixA)\n",
    "        a = np.array(matrixAT)\n",
    "        b = np.array(matrixA)\n",
    "        aTa = dot(a,b)\n",
    "        tempSVD = np.array(aTa)\n",
    "        u, s, v = svd(tempSVD)\n",
    "        s = sp.diag(s)\n",
    "        loop = v[0]\n",
    "        temp = 0\n",
    "        datas = list()\n",
    "        for i in np.arange(np.size(loop)):\n",
    "            for j in np.arange(np.size(loop)):\n",
    "                temp = temp + np.sqrt(dot(np.square(s[i][j]),np.square(v[i][j])))\n",
    "            if(temp != 0):\n",
    "               datas.append(doc[i])\n",
    "            temp = 0\n",
    "        return datas\n",
    "            \n",
    "    elif (types == 'clsa') :\n",
    "        matrixAT=result_TFIDF.toarray()\n",
    "        matrixA=np.transpose(matrixAT)\n",
    "        matrixAT=np.transpose(matrixA)\n",
    "        a = np.array(matrixAT)\n",
    "        b = np.array(matrixA)\n",
    "        aTa = dot(a,b)\n",
    "        tempSVD = np.array(aTa)\n",
    "        u, s, v = svd(tempSVD)\n",
    "        s = sp.diag(s)\n",
    "        aa =v[0]\n",
    "        for i in np.arange(np.size(aa)):\n",
    "            av = np.average(v[i])\n",
    "            for j in np.arange(np.size(aa)):\n",
    "                if v[i][j] < av :\n",
    "                    v[i][j] = 0\n",
    "\n",
    "        loop = s[0]\n",
    "        temp = 0\n",
    "        data = list()\n",
    "        for i in np.arange(np.size(loop)):\n",
    "            for j in np.arange(np.size(loop)):\n",
    "                temp = temp + np.sqrt(dot(np.square(s[i][j]),np.square(v[i][j])))\n",
    "            if(temp != 0):\n",
    "                data.append(doc[i])\n",
    "            temp = 0\n",
    "        return data\n",
    "    # except:\n",
    "    #     print(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kalimat = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pemisal_kalimat = splitParagraphIntoSentences(kalimat)\n",
    "simpan_sementara_isi_berita = list()\n",
    "berita_asli = list()\n",
    "for per_kalimat in pemisal_kalimat:\n",
    "    simpan_sementara_isi_berita.append(preprocessing(per_kalimat.strip()))\n",
    "    berita_asli.append(per_kalimat.strip())\n",
    "\n",
    "factory = StopWordRemoverFactory()\n",
    "stopwords = factory.get_stop_words()\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords,smooth_idf=False, norm=None)\n",
    "X = tfidf_vectorizer.fit_transform(simpan_sementara_isi_berita)\n",
    "return_TFIDF  = pd.DataFrame(X.toarray(), columns=tfidf_vectorizer.get_feature_names()).T\n",
    "\n",
    "return_LSA = pd.DataFrame(LSA(X)).T\n",
    "return_CLSA = pd.DataFrame(CLSA(X)).T\n",
    "\n",
    "sentences_lsa = summary_sentence(berita_asli, X, types='lsa')\n",
    "sentences_clsa = summary_sentence(berita_asli, X, types='clsa')\n",
    "\n",
    "print(return_LSA)\n",
    "print(sentences_lsa)\n",
    "\n",
    "print(return_CLSA)\n",
    "print(sentences_clsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
